\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage[hidelinks]{hyperref}

\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}
\newcommand{\deq}{\overset{\scriptscriptstyle{d}}{=}}

\newcommand{\given}{\,\vert\,}
\newcommand{\st}{\,\colon\,} % such that
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor }}
\newcommand{\one}{\mathbb{1}}

\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{exercise}{Exercise}


\begin{document}


%%%%%%%%%%%%%%
\section*{Heterozygosity}

Suppose that we have sequence from an individual,
with $C_i$ reads that map to a position overlapping site $i$, for $1 \le i \le L$,
and that the marginal distribution of $C_i$ is Poisson with mean $\lambda c_i$.
Suppose that each read covering site $i$ independently draws an allele,
with the probability of drawing allele $a$ being $p_i(a)$;
suppose that allele $a$ is observed $N_i(a)$ times amongst the $C_i$ reads at position $i$.
Given $C_i$, $N_i(\cdot)$ is Multinomial, so
on the set $\{ C_i > 1 \}$,
the probability that two reads drawn without replacement at $i$ both have $a$ is
\begin{align}
    Z_i(a) := \frac{N_i(a)(N_i(a)-1)}{C_i(C_i-1)} .
\end{align}
This has expectation
\begin{align}
\E\left[ Z_i(a) \given C_i \right] 
    & = \frac{ \E[ N_i(a)^2 - N_i(a) \given C_i ] }{ C_i (C_i-1) } \\
    & = \frac{ C_i p_i(a)(1-p_i(a)) + C_i^2 p_i^2 - C_i p_i }{ C_i (C_i-1) } \\
    % & = \frac{ C_i (C_i-1) p_i^2 }{ C_i (C_i-1) } \\
    & = p_i^2 .
\end{align}
(Note that the quantity with replacement, $(N_i(a)/C_i)^2$ does depend on $C_i$, 
since the chance of drawing the same read twice is $1/C_i$.)

Given a nonnegative function $w$ with $w(0)=w(1)=0$,
an estimator of weighted heterozygosity is
\begin{align}
    H(w) = \frac{ \sum_{i=1}^L w(C_i) \left( 1 - \sum_a Z_i(a) \right) }{ \sum_{i=1}^L w(C_i) } .
\end{align}
The expectation of $H(w)$ is therefore
\begin{align}
    \E[H(w)] &= \E \left[ \frac{ \sum_{i=1}^L w(C_i) \left( 1 - \sum_a Z_i(a) \right) }{ \sum_{i=1}^L w(C_i) } \right] \\
        &= \E\left[ \frac{ \sum_{i=1}^L w(C_i) \left( 1 - \sum_a p_i(a)^2 \right) }{ \sum_{i=1}^L w(C_i) } \right] .
\end{align}

We would like $H(w)$ to not depend on $\lambda$.
For this to be the case, we want $\lambda$ to factor out of the numerator and denominator;
ideally, we'd like
\begin{align}
    \E\left[ \frac{ \sum_i w(C_i) x_i }{ \sum_i w(C_i) } \right]
\end{align}
to not depend on $\lambda$ for any sequence $x_i$ (that may correlate with $c_i$).
If the $C_i$ are independent, this holds for $w(C_i)=C_i$, 
since by the ``coloring'' property of Poisson random variables, the expression
is equal to 
\begin{align}
    \frac{ \sum_i \lambda c_i x_i }{ \sum_i \lambda c_i }  =
    \frac{ \sum_i c_i x_i }{ \sum_i c_i }  .
\end{align}
Unfortunately, we need $w(0)=w(1)=0$, so this doesn't work.

A valid choice is $w(c)=c(c-1)$; since $\E[C_i(C_i-1)] = \lambda^2 c_i^2$,
\begin{align}
    \E\left[ \frac{ \sum_i C_i(C_i-1) x_i }{ \sum_i C_i(C_i-1) } \right] 
        &\approx \frac{ \E\left[ \sum_i C_i(C_i-1) \right] x_i }{ \E\left[ \sum_i C_i(C_i-1) \right] } \\
        &= \frac{ \sum_i c_i^2 x_i }{ \sum_i c_i^2 } .
\end{align}


%%%%%%%%%%%%%%
\section*{Divergence}

Now suppose we have sequence as above from two samples,
where $C_i^k$ is marginally Poisson with mean $\lambda_k c_i$,
and given $C_i^k$, the allele counts $N_i^k(a)$ are Multinomial
with probabilities $p_i^k(a)$.
Suppose that coverages and counts are independent between samples, given $p$ and $c$.
The probability that a pair of reads drawn from those at site $i$,
one drawn uniformly at random from each sample,
both have allele $a$ is $Y_i(a) = N_i^1(a) N_i^2(a) / C_i^1 C_i^2$,
and so on $\{C_i^1 C_i^2 > 0\}$,
\begin{align}
    \E[ Y_i(a) \given C_i^1, C_i^2 ]  =  p_i^1(a) p_i^2(a) .
\end{align}

Given a weighting function $w$ with $w(0,n)=w(n,0)=0$ for each $n$,
an estimator of divergence is 
\begin{align}
    D(w) = \frac{ \sum_{i=1}^L w(C_i^1,C_i^2) (1-\sum_a Y_i(a)) }{ \sum_{i=1}^L w(C_i^1,C_i^2) }  .
\end{align}
As above, the expection of $D(w)$ is then
\begin{align}
    \E[ D(w) ]  &= \E\left[ \frac{ \sum_{i=1}^L w(C_i^1,C_i^2) (1-\sum_a p_i^1(a) p_i^2(a)) }{ \sum_{i=1}^L w(C_i^1,C_i^2) } \right]  .
\end{align}
Also as above, the choice $w(x,y) = x y$ does not satisfy the independence property that would be ideal,
but does approximately:
\begin{align}
    \E[ D(w) ]  &\approx \frac{ \sum_{i=1}^L c_i^1 c_i^2  (1-\sum_a p_i^1(a) p_i^2(a)) }{ \sum_{i=1}^L c_i^1 c_i^2 } .
\end{align}
Another choice would be $w(x,y) = x(x-1)y(y-1)$.




%%%%%%%%%%%%%%
\section*{Mapping errors}

We can model the situation with mapping errors explicitly:
suppose that a fraction of $q_k$ of the sites in the genome
in fact have mapping multiplicity $k$.
Assuming the locations map equally well,
a pair of reads at a site of multiplicity $k$ 
in fact map to the same location with probability $1/k$.
If $\pi$ is the mean divergence between homologous sites,
$\nu$ is the mean divergence between paralogous sites,
and $\epsilon$ is the error rate,
then the mean divergence between reads at sites with multiplicity $k$ 
is $\pi/k + (1-1/k) \nu + \epsilon$.
Therefore, $D$ estimates
\begin{align}
  D \approx \epsilon + \sum_k q_k \left( \pi/k + (1-1/k) \nu \right) .
\end{align}

It might be possible to esimate $q_k$, $\pi$, and $\nu$ by looking at mean divergence as a function of coverage.


%%%%%%%%%%%%%%%%%%
\section*{Variance in heterozygosity}

Let $X$ and $Y$ be independent pairs of alleles sampled from a diploid individual
with overall heterozygosity $\pi$;
so, $X$ and $Y$ are each Bernoulli($P$), where $P=1/2$ with probability $\pi$
and $P=\epsilon$ otherwise.
Then,
\begin{align}
    \cov[X,Y] &= \frac{\pi}{4} + \epsilon(1-\pi) - \left( \frac{\pi}{2} + \epsilon(1-\pi) \right)^2 \\
        &= \pi (1-\pi) \left(\epsilon-\frac{1}{2}\right)^2 .
\end{align}
More generally,
\begin{align}
    \cov[X,Y] = \var[P] ,
\end{align}
which includes variation in error rate across sites.


\section*{Covariance of heterozygosity}

Consider a pair of haploid individuals, $x$ and $y$.
At each site $i$ on the genome, we can pick a random allele from each individuals' genotypes;
let $H_i(x,y)=1$ if the two differ (i.e., are heterozygous) and $H_i(x,y)=0$ otherwise.
We are then interested in the autocorrelation function of $H$.
This is similar to Lynch et al (\url{http://www.ncbi.nlm.nih.gov/pubmed/24948778}).

We actually have diploid individuals: pairs of haploids $(x_1,x_2)$ and $(y_1,y_2)$.
Heterozygosity is then $H(x,y) = (H(x_1,y_1)+H(x_1,y_2)+H(x_2,y_1)+H(x_2,y_2))/4$,
and 
\begin{align}
    \cov[H_i(x,y), H_j(x,y)] = \frac{1}{16} \sum_{abcd} \cov[H_i(x_a,y_b),H_j(x_c,y_d)],
\end{align}
where the sum is over choices of $a$, $b$, $c$, and $d$ from $\{1,2\}$.
Only four of the 16 terms are covariances in heterozygosities
between the same pair of chromosomes,
but the others will give us information to the extent that
the two chromosomes of an individual tend to coalesce before they coalesce with another.
(Also, since they're at the same place in space, coalescent times are correlated.)

Now fix $x$ and $y$.
Define
\begin{align}
    \pi = \frac{1}{L} \sum_i H_i .
\end{align}
We want to estimate
\begin{align}
    A(d) = \frac{1}{L-d} \sum_{i=1}^L (H_{i}-\pi)(H_{i+d}-\pi) .
\end{align}

We know already that
\begin{align}
    \E\left[ 1 - \sum_a Y_i(a) \right] = H_i ,
\end{align}
independently of $C_i$,
where $Y$ is as above and should be replaced by $Z$ if $x=y$.
The natural thing is to proceed as above,
using the weighted estimator
\begin{align}
    A_{xy}(d) = \frac{ \sum_{i=1}^L C_i^x C_i^y C_{i+d}^x C_{i+d}^y (1-\sum_a Y_i(a)-D)(1-\sum_a Y_{i+d}(a)-D)  }{\sum_{i=1}^L C_i^x C_i^y C_{i+d}^x C_{i+d}^y }. 
\end{align}
where $D$ is the estimate of $\pi$ from above.
We only need to modify this if $x=y$ and/or $d=0$.

If $x=y$ and $d>0$ then we can replace $Y$ with $Z$.

If $x=y$ and $d=0$, then we need to replace $Y_i(a)$ with
the probability that two different pairs of alleles each differ
(requiring $C_i \ge 4$),
which is 
$(1-\sum_a p_i(a)^2)^2$.
If we define
\begin{align}
    U_i &= \left(1 - \sum_a \frac{ N_i(a) (N_i(a)-1) }{ N_i (N_i-1) } \right)^2 
        + \sum_a \left\{ 
            \left( \frac{ N_i(a) (N_i(a)-1) }{ N_i (N_i-1) } \right)^2
            - \frac{ N_i(a) (N_i(a)-1) (N_i(a)-2) (N_i(a)-3) }{ N_i (N_i-1) (N_i-2) (N_i-3) } 
        \right\} \\
        &= \left(1 - \sum_a \frac{ N_i(a) (N_i(a)-1) }{ N_i (N_i-1) } \right)^2 
        + \sum_a \left( \frac{ N_i(a) (N_i(a)-1) }{ N_i (N_i-1) } \right)
        \left( \frac{ N_i(a) (N_i(a)-1) }{ N_i (N_i-1) } - \frac{ (N_i(a)-2) (N_i(a)-3) }{ (N_i-2) (N_i-3) }  \right)
\end{align}
then
\begin{align}
    \E[ U_i ] = (1-\sum_a p_i(a)^2)^2 ,
\end{align}
and so we estimate $A_{xx}(0)$ with
\begin{align}
    A_{xx}(0) = \frac{ \sum_{i=1}^L C_i (C_i-1) (C_i-2) (C_i-3) (U_i-D^2)  }{ \sum_{i=1}^L C_i (C_i-1) (C_i-2) (C_i-3) }. 
\end{align}

If $x \neq y$ but $d=0$
then we want to estimate $(1-\sum_a p_i(a)^2)(1-\sum_a p_j(a)^2)$,
the chance that two independent pairs of alleles both differ,
in each pair having one allele from $x$ and the other from $y$.
This is estimated by
\begin{align}
    V_i &= \left(1 - \sum_a \frac{ N^x_i(a) N^2_i(a) }{ N^x_i N^y_i } \right)^2
    + \sum_a 
        \left( \frac{ N^x_i(a) N^y_i(a) }{ N^x_i N^y_i } \right)
        \left(
            \frac{ N^x_i(a) N^y_i(a) }{ N^x_i N^y_i } 
            - \frac{ (N^x_i(a)-1) (N^y_i(a)-1) }{ (N^x_i-1) (N^y_i-1) } 
        \right)
\end{align}
and so we estimate $A_{xy}(0)$ with
\begin{align}
    A_{xy}(0) = \frac{ \sum_{i=1}^L C^x_i (C^x_i-1) C^y_i (C^y_i-1) (V_i-D^2) }{ \sum_{i=1}^L C^x_i (C^x_i-1) C^y_i (C^y_i-1) }. 
\end{align}

This is ignoring corrections due to the fact that $D$ is an estimator itself,
but those will be tiny.



\end{document}

