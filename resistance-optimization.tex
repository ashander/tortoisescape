\documentclass{article}
\usepackage{amsmath,amssymb}
\usepackage{fullpage}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\bone}{\mathbf{1}}

\begin{document}

Suppose that 
\begin{align}
  G = \sum_k \alpha_k A_k,
\end{align}
where $\alpha \in \R^n$
and each $A_k$ is a generator matrix,
i.e.\ $(A_k)_{xy} > 0$ for $x\neq y$ and $A_k \bone = 0$ for each $k$.

Consider a random walk driven by $G$, and let
\begin{align}
  H_{xy} = \E^x[\tau_y] = \text{ ( mean hitting time of $y$ from $x$ ) } .
\end{align}
Then  $H_{xy} \ge 0$ and
\begin{align}
  \sum_x G_{zx} H_{xy} &= -1 \quad \text{ for each } z \neq y \\
        H_{yy} = 0 .
\end{align}
Note that $\sum_x G_{yx} H_{xy} \ge 0$.

%%%%%%%%% %%%%%%%%%%
\subsection*{Gradient Ascent}

Now suppose that we are only given $T_{ij} = H_{y_i y_j}$,
and would like to find $\alpha$ to minimize the mean squared discrepancy to these.
Under a given set of $\alpha$, the vector
\begin{align}
  \hat H(\alpha)_{x y} = - \delta_x (G(\alpha)^{-y})^{-1} \bone ,
\end{align}
where $\delta_x$ is the unit vector with a $1$ in the $x$ position,
$G(\alpha) = \sum_k \alpha_k A_k$ 
and $X^{-y}$ denotes the matrix $X$ with the rows and columns corresponding to $y$ removed.
Then, we'd like to minimize
\begin{align}
  L(\alpha) = \sum_{ij} | \hat H(\alpha)_{y_i y_j} - T_{ij} |^2 .
\end{align}

Since $\partial_a X^{-1} = X^{-1} (\partial_a X )X^{-1}$,
\begin{align}
  \partial_{\alpha_k} \hat H(\alpha)_{xy} &= - \sum_z \partial_{\alpha_k} (G(\alpha)^{-y})^{-1}_{xz}  \\
  &= - \sum_z \left( (G(\alpha)^{-y})^{-1} \left( \partial_{\alpha_k} G(\alpha)^{-y} \right) (G(\alpha)^{-y})^{-1} \right)_{xz} \\
  &= - \sum_z \left( (G(\alpha)^{-y})^{-1} (A_k)^{-y} (G(\alpha)^{-y})^{-1} \right)_{xz} \\
  &= - \delta_x (G(\alpha)^{-y})^{-1} (A_k)^{-y} (G(\alpha)^{-y})^{-1} \bone \\
  &= \delta_x (G(\alpha)^{-y})^{-1} (A_k)^{-y} \hat H(\alpha)_{\cdot y},
\end{align}
where $X_{\cdot y_j}$ is the $y_j$ column of $X$,
and so the gradient of $L$ is
\begin{align}
  \partial_{\alpha_k} L(\alpha) &= \sum_{ij} 2 \left( \partial_{\alpha_k} \hat H(\alpha)_{y_i y_j} \right) \left( \hat H(\alpha)_{y_i y_j} - T_{ij} \right) \\
  &= \sum_{ij} 2 \left( \partial_{\alpha_k} \hat H(\alpha)_{y_i y_j} \right) \left( \hat H(\alpha)_{y_i y_j} - T_{ij} \right) \\
  &= -2 \sum_{ij} \left( \delta_{y_i} (G(\alpha)^{-y_j})^{-1} (A_k)^{-y_j} \hat H(\alpha)_{\cdot y_j} \right) \left( \hat H(\alpha)_{y_i y_j} - T_{ij} \right)
\end{align}
If we let
\begin{align}
  D(\alpha)_{ij} = - \left( \delta_{y_i} (G(\alpha)^{-y_j})^{-1} (A_k)^{-y_j} (G(\alpha)^{-y_j})^{-1} \bone \right)
\end{align}
and abuse notation to treat $\hat H(\alpha)_{ij}$ as an $m \times m$ matrix, then this is
\begin{align}
  \partial_{\alpha_k} L(\alpha) &= \sum_{ij} ( \hat H(\alpha)_{ij} - T_{ij} ) D(\alpha)_{ij}
\end{align}

\paragraph{Note:}
In computing the gradient, we can re-use computation of $\hat H$,
but still have to solve another linear equation involving $G(\alpha)$,
for each layer.
Also note: requires us to use the full square, symmetric matrix of $H$;
we could use a rectangular subset of it, reducing the number of inversions that need to be done.


%%%%%% %%%%%%%%
\subsection*{Given full data}

Now suppose we are given the $A_k$ and $H_{xy_i}$ for all $x$ and some set of $\{y_i\}_{i=1}^m$.
(Not what we actually have, but carry on.)
We could find $\alpha$ to minimize
\begin{align}
  F(\alpha) = \sum_{i=1}^m \sum_{z \neq y_i} \left( \sum_k \alpha_k \sum_x (A_k)_{zx} H_{xy_i} + 1 \right)^2 .
\end{align}
Differentiating this with respect to $\alpha_\ell$,
we get
\begin{align}
  \partial_{\alpha_\ell} F(\alpha) = 2 \sum_{i=1}^m \sum_{z \neq y_i} \left( \sum_x (A_\ell)_{zx} H_{xy_i} \right) \left( \sum_k \alpha_k \sum_x (A_k)_{zx} H_{xy_i} + 1 \right) .
\end{align}
For each $i$ and $\ell$ let $B^{i\ell}$ be the vector
\begin{align}
  B^{i\ell}_z = \sum_x (A_\ell)_{zx} H_{xy_i} ,
\end{align}
and let $B^{i\ell} \cdot B^{ik} = \sum_{z \neq y_i} B^{i\ell}_z B^{ik}_z$, etcetera.
The derivative above is then
\begin{align}
  \partial_{\alpha_\ell} F(\alpha) = 2 \sum_{i=1}^m \left( \sum_k \alpha_k B^{i\ell} \cdot B^{ik} + B^{i\ell} \cdot \bone \right) .
\end{align}
Setting these to zero for each $\ell$ results in the system of linear equations
\begin{align}
  Q \alpha = b
\end{align}
where
\begin{align}
    Q_{jk} &= \sum_{i=1}^m  B^{i\ell} \cdot B^{ik} \\
    b_\ell  &= - \sum_{i=1}^m  B^{i\ell } \cdot \bone .
\end{align}

For computation, we could compute the matrix
\begin{align}
  B^\ell  = A_\ell  H ,
\end{align}
indexed by $z$ and $i$,
and then set
\begin{align}
  B^\ell _{y_i i} := 0 \quad \text{for each } i ,
\end{align}
and then $C^\ell  = B^\ell  \bone$, i.e.
\begin{align}
  C^\ell _z = \sum_{i=1}^m B^\ell_{z i} .
\end{align}
Then we would have
\begin{align}
    Q_{\ell k} &= C_\ell \cdot C_k \\
    b_\ell &= C_\ell \cdot \bone' .
\end{align}


%%%%%%%% %%%%%%%%%
\subsection*{Interpolation}

Of course, we only have $T$, not $H$.
One approach would be to interpolate $T$ to get an approximation of $H$,
then proceed as above, pretending we have the correct $H$.
Then, given estimates of $\alpha$, we could possibly improve our interpolation,
and iterate.

To that end, suppose that we are given $G$ and $T$ and estimate $H$ by:
\begin{align}
  \tilde H_{x,y_i} &= \sum_{k=1}^K \lambda_{ik} v_{k}(x-y_i) \\
  &= \sum_{k=1}^K \lambda_{ik} v_{ik}(x) ,
\end{align}
where the $v_{ik}$ are given, and we want to choose $\lambda_{ik}$ to minimize
\begin{align}
  I(\lambda) &= \sum_{i=1}^m \sum_{x \neq y_i} \left( \sum_z G_{xz} \tilde H_{z y_i} + 1 \right)^2  + \gamma \sum_{ji} ( \tilde H(y_j,y_i) - T_{ji} )^2\\
  &=\sum_{i=1}^m \sum_{x \neq y_i} \left( \sum_{k=1}^K \lambda_{ik} (Gv_{ik})_{x y_i}  + 1 \right)^2  
  + \gamma \sum_{ji} ( \sum_{k=1}^K \lambda_{ik} \left(Gv_{ik})_{y_j y_i} - T_{ji} \right)^2,
\end{align}
where $\gamma$ should be strong enough that $\tilde H_{y_j,y_i}$ is close to $T_{ji}$.
Then
\begin{align}
  \partial_{\lambda_{uv}} I(\lambda)  &=
  2 \sum_{x \neq y_u} ( G v_{uv} )_{x y_u} \left( \sum_{k=1}^K \lambda_{uk} (Gv_{uk})_{x y_u}  + 1 \right)  
  + 2 \gamma \sum_{j} (G v_{uv})_{y_v y_u} \left( \sum_{k=1}^K \lambda_{uk} (Gv_{uk})_{y_j y_u} - T_{ju} \right)  \\
  &= 2 \sum_{k=1}^K \lambda_{uk} \left\{ 
  \sum_{x \neq y_u} ( G v_{uv} )_{x y_u} (Gv_{uk})_{x y_u}   
  + \gamma \sum_{j} (G v_{uv})_{y_v y_u} (Gv_{uk})_{y_j y_u}  \right\} \\
  & \qquad {}
  + 2 \sum_{x \neq y_u} ( G v_{uv} )_{x y_u} - 2 \gamma \sum_{j} (G v_{uv})_{y_v y_u} T_{ju} 
\end{align}
Note that the equations that $\lambda$ solve decouple across $u$, 
so there are $m$ systems of $K$ linear equations that need to be solved.
Also note that if we precompute the $A_\ell v_{ik}$ then we can work directly with the $\alpha$ and $\lambda$.


\paragraph{Even simpler,}
we could solve the following problem:
define $P$ to be the projection matrix so that $T = PH$.
Then for each $i$ we want to solve the following problem:
\begin{align}
    \text{minimize } & \| Q^{-y_i} z + \bone \|^2 + \gamma \| Pz - H_{\cdot y_i} \|^2 \\
    \text{subject to } &  z \ge 0 .
\end{align}

This works ridiculously well on things closeish to true hitting times,
not even requiring the constraint.

In some cases, so does solving:
\begin{align}
    \text{minimize } & \| Q^{-y_i} z \|^2 + \gamma \| Pz - H_{\cdot y_i} \|^2 \\
    \text{subject to } &  z \ge 0 .
\end{align}
This is because the function that is harmonic off of $\{y_j\}$
and equal to the mean hitting time of $y_i$ at those locations
is very nearly the minimal solution to this problem.
This only underestimates the truth by the mean time to first hit one of the $\{y_j\}$.



%%%%%%%% %%%%%%%%%%%%
\subsection*{More general resistances}

We probably don't want migration rates to be proportional to the landscape layers,
at least in all cases.
Here's a general model:
Letting $L$ be the landscape layers,
suppose that we have scalars $\gamma$ and $\delta$, a positive function $\rho$, and a nonnegative symmetric function $g$ such that
\begin{align}
  \gamma L_x &:= \gamma_1 L^1_x \cdots + \gamma_m L^m_x  \\
  \delta L_x &:= \delta_1 L^1_x \cdots + \delta_m L^m_x  \\
  G_{xy} &= \rho( \gamma L_x ) g( \delta L_x, \delta L_y ) \quad \text{for $x \sim y$} \\
  G_{xx} &= - \sum_{y \sim x} \rho( \gamma L_x ) g( \delta L_x, \delta L_y ) ,
\end{align}
where $x \sim y$ denotes that $x$ and $y$ are adjacent (and not equal).
Note that this chain is reversible with stationary distribution proportional to $1/\rho$,
and that
\begin{align}
  J_{xy} = \frac{1}{\sqrt{\rho(\gamma L_x)}} G_{xy} \sqrt{ \rho(\gamma L_y) }
\end{align}
is symmetric, and hence more numerically tractable (should be used in the interpolation problem above, for instance).

Recall, that here we are modeling the movement of lineages both forwards and backwards through time.
(Somehow, their average?)
Therefore, we should probably use a reversible model.
The stationary distribution $\rho$ is proportional to the goodness of the habitat
(measured by long-term fitness)
and the edge weights $g$ say how good the habitat is to move through.

Note that
\begin{align}
  \partial_\gamma G_{xy} &= L_x \rho'( \gamma L_x ) g( \delta L_x, \delta L_y ) \\
  \partial_\delta G_{xy} &= \rho( \gamma L_x ) \left( L_x g'( \delta L_x, \delta L_y )  + L_y g'( \delta L_x, \delta L_y ) \right ) 
      \quad \text{for } x \neq y \\
      \partial_\delta G_{xx} &=  - \rho( \gamma L_x ) \left( L_x \sum_{y \sim x} g'(\delta L_x, \delta L_y) + \sum_{y \sim x} L_y g'( \delta L_x, \delta L_y ) \right) \quad \text{for } x \neq y \\
      &= - \sum_{y \sim x} \partial_\delta G_{xy} 
\end{align}

Then, again with
\begin{align}
  F(\gamma,\delta) = \sum_{i=1}^m \sum_{z \neq y_i} \left( \sum_x G_{zx} H_{xy} + 1 \right)^2 
\end{align}
and
\begin{align}
  \partial_{\gamma_k} F(\gamma) &= 2 \sum_{i=1}^m \sum_{z \neq y_i} \left( L^k_z \rho'(\gamma L_z) \sum_{x \neq z} g(\delta L_z, \delta L_x) (H_{xy}-H_{zy}) \right) \left( \sum_x G_{zx} H_{xy} + 1 \right)  
\end{align}
and
\begin{align}
  \partial_{\delta_k} F(\gamma) &= 2 \sum_{i=1}^m \sum_{z \neq y_i} 
    \left( \sum_x G_{zx} H_{xy} + 1 \right) 
  \left( \rho(\gamma L_z) 
    \sum_{x \neq z} (L^k_z + L^k_x) g'(\delta L_z, \delta L_x) ( H_{xy} - H_{zy} ) 
  \right) 
  % \rho(\gamma L_z) \left( 
  %   \sum_{x \neq z} (L^k_z + L^k_x) g'(\delta L_z, \delta L_x) H_{xy}
  %   \right. \\ & \qquad \qquad \left.
  %   - \left(L^k_z \sum_{w \sim z} g'(\delta L_z, \delta L_w) 
  %   + \sum_{w \sim z} L^k_w g'(\delta L_z, \delta L_w) \right) H_{zy} 
  % \right) 
\end{align}


\paragraph{Exponential transforms}

Now suppose that
\begin{align}
  \rho(u) &= e^{u} \\
  g(u,v) &= e^{u+v} .
\end{align}
This is probably the simplest case, but because the exponential gets very steep,
might run into numerical difficulties.

Then
\begin{align}
  \partial_{\gamma_k} F(\gamma) &= 
    2 \sum_{i=1}^m \sum_{z \neq y_i} 
    \left( L^k_z \sum_x G_{zx} H_{xy}\right) 
    \left( \sum_x G_{zx} H_{xy} + 1 \right)  
\end{align}
and
\begin{align}
  \partial_{\delta_k} F(\gamma) &= 
    2 \sum_{i=1}^m \sum_{z \neq y_i} 
    \left( L^k_z \sum_x G_{zx} H_{xy}
      + \sum_{x \neq z} G_{zx} L^k_x H_{xy}
      + G_{zz} ( \sum_{w \sim z} L^k_w )  H_{zy}
    \right) 
    \left( \sum_x G_{zx} H_{xy} + 1 \right)  \\
\end{align}


\paragraph{Logistic transforms}

Now suppose that
\begin{align}
  \rho(u) &= \frac{1}{1+e^{-u}}  \\
  g(u,v) &= \frac{1}{1+e^{-(u+v)}} = \rho(u+v) .
\end{align}
This flattens out, which means that it might be insensitive to some of the $\gamma$ or $\delta$,
but even so, what we care about more is $\rho$ and $g$, which will be well-determined.

Note that $\rho'(u) = \rho(u) (1-\rho(u))$,
so that
\begin{align}
  \partial_{\gamma_k} F(\gamma) &= 2 \sum_{i=1}^m \sum_{z \neq y_i} \left( L^k_z (1-\rho(\gamma L_z)) G_{zx} H_{xy}\right) \left( \sum_x G_{zx} H_{xy} + 1 \right)  
\end{align}
and
\begin{align}
  \partial_{\delta_k} F(\gamma) &= 2 \sum_{i=1}^m \sum_{z \neq y_i} \left( \sum_{x \neq z} (L^k_z + L^k_x) (1-\rho(\delta(L_z+L_x))) G_{zx} (H_{xy}-H_{zy}) \right) \left( \sum_x G_{zx} H_{xy} + 1 \right)  \\
\end{align}

\end{document}


%%%%%%%% %%%%%%%% 
\section{Initial values}
 
It would be nice to get order-of-magnitude estimates
for where to start the parameters above.
To do this, we can pretend that space is flat.
Let $\tau_r$ be the first hitting time of two-dimensional Brownian motion of a disk of radius $r$.
Then with the motion begun at radius $x > r$,
\begin{align}
  \E_x[ e^{-\alpha \tau_r} ] = \frac{ K_0( x \sqrt{2 \alpha } ) }{ K_0( r \sqrt{2 \alpha } ) } ,
\end{align}
where $K_0(x)$ is the modified Bessel function of the second kind (Borodin \& Salminen 4.2.0.1).
Differentiating, using Gradshteyn \& Ryzhik 8.486.13 and 8.446,
that $\partial_z K_0(z) = - K_1(z)$,
that $K_0(x) \sim \log(1/x)$ for small $x$,
and $K_1(x) \sim 1/x$ for small $x$,
\begin{align}
  \E_x[ \tau_r ] &= - \partial_\alpha \frac{ K_0( x \sqrt{2 \alpha } ) }{ K_0( r \sqrt{2 \alpha } ) } \Bigg \vert_{\alpha=0} \\
  &= \lim_{\alpha \to 0} \frac{ K_1( x \sqrt{2 \alpha} ) K_0(r \sqrt{2 \alpha}) - K_0(x \sqrt{2\alpha}) K_1( r \sqrt{2 \alpha} ) }{ K_0(r \sqrt{2\alpha} )^2 }  \\
  &= \lim_{\alpha \to 0}  \frac{ \log(x \sqrt{2 \alpha}) / ( r \sqrt{2 \alpha} ) - \log(r \sqrt{2 \alpha}) / ( x \sqrt{2 \alpha} ) }{ \log( r \sqrt{2\alpha} )^2 } \\
\end{align}

Now suppose that typical points in the range are separated by $R$
and that our discretization divides regions into pieces of width $\epsilon$.
Then the mean hitting time 

