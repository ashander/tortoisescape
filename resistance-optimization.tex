\documentclass{article}

\usepackage{amsmath,amssymb}
\usepackage{fullpage}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\bone}{\mathbf{1}}

\begin{document}

\section{Linear jump rates}

Suppose that 
\begin{align}
  G = \sum_k \alpha_k A_k,
\end{align}
where $\alpha \in \R^n$
and each $A_k$ is a generator matrix,
i.e.\ $(A_k)_{xy} > 0$ for $x\neq y$ and $A_k \bone = 0$ for each $k$.

Consider a random walk driven by $G$, and let
\begin{align}
  H_{xy} = \E^x[\tau_y] = \text{ ( mean hitting time of $y$ from $x$ ) } .
\end{align}
Then  $H_{xy} \ge 0$ and
\begin{align}
  \sum_x G_{zx} H_{xy} &= -1 \quad \text{ for each } z \neq y \\
        H_{yy} = 0 .
\end{align}
Note that $\sum_x G_{yx} H_{xy} \ge 0$.

%%%%%%%%% %%%%%%%%%%
\subsection*{Gradient Ascent}

Now suppose that we are only given $T_{ij} = H_{y_i y_j}$,
and would like to find $\alpha$ to minimize the mean squared discrepancy to these.
Under a given set of $\alpha$, the vector
\begin{align}
  \hat H(\alpha)_{x y} = - \delta_x (G(\alpha)^{-y})^{-1} \bone ,
\end{align}
where $\delta_x$ is the unit vector with a $1$ in the $x$ position,
$G(\alpha) = \sum_k \alpha_k A_k$ 
and $X^{-y}$ denotes the matrix $X$ with the rows and columns corresponding to $y$ removed.
Then, we'd like to minimize
\begin{align}
  L(\alpha) = \sum_{ij} | \hat H(\alpha)_{y_i y_j} - T_{ij} |^2 .
\end{align}

Since $\partial_a X^{-1} = X^{-1} (\partial_a X )X^{-1}$,
\begin{align}
  \partial_{\alpha_k} \hat H(\alpha)_{xy} &= - \sum_z \partial_{\alpha_k} (G(\alpha)^{-y})^{-1}_{xz}  \\
  &= - \sum_z \left( (G(\alpha)^{-y})^{-1} \left( \partial_{\alpha_k} G(\alpha)^{-y} \right) (G(\alpha)^{-y})^{-1} \right)_{xz} \\
  &= - \sum_z \left( (G(\alpha)^{-y})^{-1} (A_k)^{-y} (G(\alpha)^{-y})^{-1} \right)_{xz} \\
  &= - \delta_x (G(\alpha)^{-y})^{-1} (A_k)^{-y} (G(\alpha)^{-y})^{-1} \bone \\
  &= \delta_x (G(\alpha)^{-y})^{-1} (A_k)^{-y} \hat H(\alpha)_{\cdot y},
\end{align}
where $X_{\cdot y_j}$ is the $y_j$ column of $X$,
and so the gradient of $L$ is
\begin{align}
  \partial_{\alpha_k} L(\alpha) &= \sum_{ij} 2 \left( \partial_{\alpha_k} \hat H(\alpha)_{y_i y_j} \right) \left( \hat H(\alpha)_{y_i y_j} - T_{ij} \right) \\
  &= \sum_{ij} 2 \left( \partial_{\alpha_k} \hat H(\alpha)_{y_i y_j} \right) \left( \hat H(\alpha)_{y_i y_j} - T_{ij} \right) \\
  &= -2 \sum_{ij} \left( \delta_{y_i} (G(\alpha)^{-y_j})^{-1} (A_k)^{-y_j} \hat H(\alpha)_{\cdot y_j} \right) \left( \hat H(\alpha)_{y_i y_j} - T_{ij} \right)
\end{align}
If we let
\begin{align}
  D(\alpha)_{ij} = - \left( \delta_{y_i} (G(\alpha)^{-y_j})^{-1} (A_k)^{-y_j} (G(\alpha)^{-y_j})^{-1} \bone \right)
\end{align}
and abuse notation to treat $\hat H(\alpha)_{ij}$ as an $m \times m$ matrix, then this is
\begin{align}
  \partial_{\alpha_k} L(\alpha) &= \sum_{ij} ( \hat H(\alpha)_{ij} - T_{ij} ) D(\alpha)_{ij}
\end{align}

\paragraph{Note:}
In computing the gradient, we can re-use computation of $\hat H$,
but still have to solve another linear equation involving $G(\alpha)$,
for each layer.
Also note: requires us to use the full square, symmetric matrix of $H$;
we could use a rectangular subset of it, reducing the number of inversions that need to be done.


%%%%%% %%%%%%%%
\subsection*{Given full data}

Now suppose we are given the $A_k$ and $H_{xy_i}$ for all $x$ and some set of $\{y_i\}_{i=1}^m$.
(Not what we actually have, but carry on.)
We could find $\alpha$ to minimize
\begin{align}
  F(\alpha) = \sum_{i=1}^m \sum_{z \neq y_i} \left( \sum_k \alpha_k \sum_x (A_k)_{zx} H_{xy_i} + 1 \right)^2 .
\end{align}
Differentiating this with respect to $\alpha_\ell$,
we get
\begin{align}
  \partial_{\alpha_\ell} F(\alpha) = 2 \sum_{i=1}^m \sum_{z \neq y_i} \left( \sum_x (A_\ell)_{zx} H_{xy_i} \right) \left( \sum_k \alpha_k \sum_x (A_k)_{zx} H_{xy_i} + 1 \right) .
\end{align}
For each $i$ and $\ell$ let $B^{i\ell}$ be the vector
\begin{align}
  B^{i\ell}_z = \sum_x (A_\ell)_{zx} H_{xy_i} ,
\end{align}
and let $B^{i\ell} \cdot B^{ik} = \sum_{z \neq y_i} B^{i\ell}_z B^{ik}_z$, etcetera.
The derivative above is then
\begin{align}
  \partial_{\alpha_\ell} F(\alpha) = 2 \sum_{i=1}^m \left( \sum_k \alpha_k B^{i\ell} \cdot B^{ik} + B^{i\ell} \cdot \bone \right) .
\end{align}
Setting these to zero for each $\ell$ results in the system of linear equations
\begin{align}
  Q \alpha = b
\end{align}
where
\begin{align}
    Q_{jk} &= \sum_{i=1}^m  B^{i\ell} \cdot B^{ik} \\
    b_\ell  &= - \sum_{i=1}^m  B^{i\ell } \cdot \bone .
\end{align}

For computation, we could compute the matrix
\begin{align}
  B^\ell  = A_\ell  H ,
\end{align}
indexed by $z$ and $i$,
and then set
\begin{align}
  B^\ell _{y_i i} := 0 \quad \text{for each } i ,
\end{align}
and then $C^\ell  = B^\ell  \bone$, i.e.
\begin{align}
  C^\ell _z = \sum_{i=1}^m B^\ell_{z i} .
\end{align}
Then we would have
\begin{align}
    Q_{\ell k} &= C_\ell \cdot C_k \\
    b_\ell &= C_\ell \cdot \bone' .
\end{align}


%%%%%%%% %%%%%%%%%
\subsection*{Interpolation}

Of course, we only have $T$, not $H$.
One approach would be to interpolate $T$ to get an approximation of $H$,
then proceed as above, pretending we have the correct $H$.
Then, given estimates of $\alpha$, we could possibly improve our interpolation,
and iterate.

To that end, suppose that we are given $G$ and $T$ and estimate $H$ by:
\begin{align}
  \tilde H_{x,y_i} &= \sum_{k=1}^K \lambda_{ik} v_{k}(x-y_i) \\
  &= \sum_{k=1}^K \lambda_{ik} v_{ik}(x) ,
\end{align}
where the $v_{ik}$ are given, and we want to choose $\lambda_{ik}$ to minimize
\begin{align}
  I(\lambda) &= \sum_{i=1}^m \sum_{x \neq y_i} \left( \sum_z G_{xz} \tilde H_{z y_i} + 1 \right)^2  + \gamma \sum_{ji} ( \tilde H(y_j,y_i) - T_{ji} )^2\\
  &=\sum_{i=1}^m \sum_{x \neq y_i} \left( \sum_{k=1}^K \lambda_{ik} (Gv_{ik})_{x y_i}  + 1 \right)^2  
  + \gamma \sum_{ji} ( \sum_{k=1}^K \lambda_{ik} \left(Gv_{ik})_{y_j y_i} - T_{ji} \right)^2,
\end{align}
where $\gamma$ should be strong enough that $\tilde H_{y_j,y_i}$ is close to $T_{ji}$.
Then
\begin{align}
  \partial_{\lambda_{uv}} I(\lambda)  &=
  2 \sum_{x \neq y_u} ( G v_{uv} )_{x y_u} \left( \sum_{k=1}^K \lambda_{uk} (Gv_{uk})_{x y_u}  + 1 \right)  
  + 2 \gamma \sum_{j} (G v_{uv})_{y_v y_u} \left( \sum_{k=1}^K \lambda_{uk} (Gv_{uk})_{y_j y_u} - T_{ju} \right)  \\
  &= 2 \sum_{k=1}^K \lambda_{uk} \left\{ 
  \sum_{x \neq y_u} ( G v_{uv} )_{x y_u} (Gv_{uk})_{x y_u}   
  + \gamma \sum_{j} (G v_{uv})_{y_v y_u} (Gv_{uk})_{y_j y_u}  \right\} \\
  & \qquad {}
  + 2 \sum_{x \neq y_u} ( G v_{uv} )_{x y_u} - 2 \gamma \sum_{j} (G v_{uv})_{y_v y_u} T_{ju} 
\end{align}
Note that the equations that $\lambda$ solve decouple across $u$, 
so there are $m$ systems of $K$ linear equations that need to be solved.
Also note that if we precompute the $A_\ell v_{ik}$ then we can work directly with the $\alpha$ and $\lambda$.


\paragraph{Even simpler,}
we could solve the following problem:
define $P$ to be the projection matrix so that $T = PH$.
Then for each $i$ we want to solve the following problem:
\begin{align}
    \text{minimize } & \| Q^{-y_i} z + \bone \|^2 + \gamma \| Pz - H_{\cdot y_i} \|^2 \\
    \text{subject to } &  z \ge 0 .
\end{align}

This works ridiculously well on things closeish to true hitting times,
not even requiring the constraint.

In some cases, so does solving:
\begin{align}
    \text{minimize } & \| Q^{-y_i} z \|^2 + \gamma \| Pz - H_{\cdot y_i} \|^2 \\
    \text{subject to } &  z \ge 0 .
\end{align}
This is because the function that is harmonic off of $\{y_j\}$
and equal to the mean hitting time of $y_i$ at those locations
is very nearly the minimal solution to this problem.
This only underestimates the truth by the mean time to first hit one of the $\{y_j\}$.

Note that in moving from solving the problem $z = - (Q^{-y})^{-1} \bone$ to the problem $ Q^{-y} z = -1$
we have mulitplied by $Q^{-y}$,
a matrix that is invertible,
but only barely,
having top eigenvalue equal to the rate of hitting of $y$ from the quasistationary distribution off of $y$,
and associated eigenvector almost constant.
This suggests that solving the problem by minimizing $ \| Q^{-y_i} z \|^2$
would benefit from an additional constraint on the absolute magnitude of $z$, somehow;
fortunately, the constraint that $Pz$ be close to the observed values provides such a constraint.


%%%%%%%% %%%%%%%%%%%%
\section*{More general relationships}

We probably don't want migration rates to be proportional to the landscape layers,
at least in all cases.
Here's a general model:
Letting $L$ be the landscape layers,
suppose that we have scalars $\gamma$ and $\delta$, 
a scalar $\beta$,
a positive function $\rho$, 
and a nonnegative symmetric function $g$ such that
\begin{align}
  \gamma L_x &:= \gamma_1 L^1_x \cdots + \gamma_m L^m_x  \\
  \delta L_x &:= \delta_1 L^1_x \cdots + \delta_m L^m_x  \\
  G_{xy} &= e^\beta \rho( \gamma L_x ) g( \delta L_x, \delta L_y ) \quad \text{for $x \sim y$} \\
  G_{xx} &= - e^\beta \sum_{y \sim x} \rho( \gamma L_x ) g( \delta L_x, \delta L_y ) ,
\end{align}
where $x \sim y$ denotes that $x$ and $y$ are adjacent (and not equal),
so that $G_{xz} = 0$ for any $x$ and $z$ that are not adjacent.
Note that this chain is reversible with stationary distribution proportional to $1/\rho$,
and that
\begin{align}
  J_{xy} = \frac{1}{\sqrt{\rho(\gamma L_x)}} G_{xy} \sqrt{ \rho(\gamma L_y) }
\end{align}
is symmetric, and hence more numerically tractable (should be used in the interpolation problem above, for instance).

Recall, that here we are modeling the movement of lineages both forwards and backwards through time.
(Somehow, their average?)
Therefore, we should probably use a reversible model.
The stationary distribution $\rho$ is proportional to the goodness of the habitat
(measured by long-term fitness)
and the edge weights $g$ say how good the habitat is to move through.

Note that
\begin{align}
  \partial_\beta G_{xy} &= e^\beta \rho( \gamma L_x ) g( \delta L_x, \delta L_y )  = G_{xy}\\
  \partial_\gamma G_{xy} &= e^\beta L_x \rho'( \gamma L_x ) g( \delta L_x, \delta L_y ) \\
  \partial_\delta G_{xy} &= e^\beta \rho( \gamma L_x ) \left( L_x g'( \delta L_x, \delta L_y )  + L_y g'( \delta L_x, \delta L_y ) \right ) 
      \quad \text{for } x \neq y \\
  \partial_\delta G_{xx} &=  - e^\beta \rho( \gamma L_x ) \left( L_x \sum_{y \sim x} g'(\delta L_x, \delta L_y) + \sum_{y \sim x} L_y g'( \delta L_x, \delta L_y ) \right) \quad \text{for } x \neq y \\
      &= - \sum_{y \sim x} \partial_\delta G_{xy} 
\end{align}

Then, again with
\begin{align}
  F(\beta,\gamma,\delta) = \sum_{i=1}^m \sum_{z \neq y_i} \left( \sum_x G_{zx} H_{xy} + 1 \right)^2 
\end{align}
we have
\begin{align}
    \partial_\beta F(\beta,\gamma,\delta) &= 2 \sum_{i=1}^m \sum_{z \neq y_i} \left( \sum_x G_{zx} H_{xy} \right)\left( \sum_x G_{zx} H_{xy} + 1 \right)
\end{align}
and
\begin{align}
  \partial_{\gamma_k} F(\beta,\gamma,\delta) &= 2 \sum_{i=1}^m \sum_{z \neq y_i} \left( L^k_z e^\beta \rho'(\gamma L_z) \sum_{x \neq z} g(\delta L_z, \delta L_x) (H_{xy}-H_{zy}) \right) \left( \sum_x G_{zx} H_{xy} + 1 \right)  
\end{align}
and
\begin{align}
  \partial_{\delta_k} F(\beta,\gamma,\delta) &= 2 \sum_{i=1}^m \sum_{z \neq y_i} 
    \left( \sum_x G_{zx} H_{xy} + 1 \right) 
  \left( e^\beta \rho(\gamma L_z) 
    \sum_{x \neq z} (L^k_z + L^k_x) g'(\delta L_z, \delta L_x) ( H_{xy} - H_{zy} ) 
  \right) 
  % \rho(\gamma L_z) \left( 
  %   \sum_{x \neq z} (L^k_z + L^k_x) g'(\delta L_z, \delta L_x) H_{xy}
  %   \right. \\ & \qquad \qquad \left.
  %   - \left(L^k_z \sum_{w \sim z} g'(\delta L_z, \delta L_w) 
  %   + \sum_{w \sim z} L^k_w g'(\delta L_z, \delta L_w) \right) H_{zy} 
  % \right) 
\end{align}



\paragraph{Exponential transforms}

Now suppose that
\begin{align}
  \rho(u) &= e^{u} \\
  g(u,v) &= e^{u+v} .
\end{align}
This is probably the simplest case, but because the exponential gets very steep,
might run into numerical difficulties.

Then
\begin{align} \label{eqn:expl_deriv_gamma}
  \partial_{\gamma_k} F(\beta,\gamma,\delta) &= 
    2 \sum_{i=1}^m \sum_{z \neq y_i} 
    \left( L^k_z \sum_x G_{zx} H_{xy_i}\right) 
    \left( \sum_x G_{zx} H_{xy_i} + 1 \right)  
\end{align}
and
\begin{align} \label{eqn:expl_deriv_delta}
  \partial_{\delta_k} F(\beta,\gamma,\delta) &= 
    2 \sum_{i=1}^m \sum_{z \neq y_i} 
    \left( L^k_z \sum_x G_{zx} H_{xy_i}
      + \sum_{x \neq z} G_{zx} L^k_x H_{xy_i}
      + G_{zz} ( \sum_{w \sim z} L^k_w )  H_{zy_i}
    \right) 
    \left( \sum_x G_{zx} H_{xy_i} + 1 \right)  \\
\end{align}


\paragraph{Logistic transforms}

Now suppose that
\begin{align}
  \rho(u) &= \frac{1}{1+e^{-u}}  \\
  g(u,v) &= \frac{1}{1+e^{-(u+v)}} = \rho(u+v) .
\end{align}
This flattens out, which means that it might be insensitive to some of the $\gamma$ or $\delta$,
but even so, what we care about more is $\rho$ and $g$, which will be well-determined.

Note that $\rho'(u) = \rho(u) (1-\rho(u))$,
so that
\begin{align}
  \partial_{\gamma_k} F(\beta,\gamma,\delta) 
  &= 2 \sum_{i=1}^m \sum_{z \neq y_i} \left( L^k_z (1-\rho(\gamma L_z)) \sum_x G_{zx} H_{xy_i}\right) \left( \sum_x G_{zx} H_{xy_i} + 1 \right)  
\end{align}
and
\begin{align}
  \partial_{\delta_k} F(\beta,\gamma,\delta) &= 2 \sum_{i=1}^m \sum_{z \neq y_i} \left( \sum_{x \neq z} (L^k_z + L^k_x) (1-\rho(\delta(L_z+L_x))) G_{zx} (H_{xy_i}-H_{zy_i}) \right) \left( \sum_x G_{zx} H_{xy_i} + 1 \right)  \\
\end{align}


%%%%%%%%%%%%%%
\section{Derivative and curvature}

\subsection{A simplifying observation}

For the moment, fix $A_j$, and work with only vectors with $A_j$ removed;
letting $\bar G = G^{-A_j}$, and $\bar H_x = H_{x,A_j}^{-A_j}$.
\begin{align}
  \partial_\theta ( \bar G \bar H + 1 )_i^2 &= 2 ( \partial_\theta \bar G \bar H )_i (\bar G \bar H +1)_i \\
  \partial_\theta \partial_\phi ( \bar G \bar H + 1 )_i^2 
    &= 2 \left\{ ( \partial_\theta \partial_\phi \bar G \bar H )_i (\bar G \bar H +1)_i 
      ( \partial_\theta \bar G \bar H )_i ( \partial_\phi \bar G \bar H )_i \right\}
\end{align}
and as above, if
\begin{align}
  G_{xy} = e^\beta \rho(\gamma A_x) \rho(\delta B_{xy})
\end{align}
(where $B$ is symmetric, but no matter)
then
\begin{align}
  \partial_\beta G_{xy} &= G_{xy} \\
  \partial_\gamma G_{xy} &= A_x (1-\rho(\gamma A_x)) G_{xy} \\
  \partial_\delta G_{xy} &= B_{xy} (1-\rho(\delta B_{xy})) G_{xy}  .
\end{align}
and also 
\begin{align}
  \partial_\beta \partial_\gamma G_{xy} &= \partial_\gamma G_{xy} \\
  \partial_\beta \partial_\delta G_{xy} &= \partial_\delta G_{xy} \\
  \partial_\gamma \partial_\delta G_{xy} &= A_x (1-\rho(\gamma A_x)) B_{xy} (1-\rho(\delta B_{xy})) G_{xy}  .
\end{align}


\subsection{Matching hitting times, again}

Let us return to the situation we started with initially:
find parameters to minimize
\begin{align}
  L = \sum_{ij} \left( \hat H_{x_i,A_j} - T_{ij} \right)^2 .
\end{align}

Now, recall that
\begin{align}
  H_{x,A_j} = \begin{cases}
    (G^{-A_j})^{-1} \bone \quad & \text{if } x \notin A_j \\
    0 \quad & \text{otherwise.}
  \end{cases}
\end{align}
For the moment, fix $A_j$, and work with only vectors with $A_j$ removed;
letting $\bar G = G^{-A_j}$, and $\bar H_x = H_{x,A_j}^{-A_j}$.
Then
\begin{align}
  \partial_\theta \bar H_x 
  &= - \delta_x \bar G^{-1} (\partial_\theta \bar G) \bar G^{-1} (-\bone) \\
  &= - \delta_x \bar G^{-1} (\partial_\theta \bar G) \bar H ,
\end{align}
and
\begin{align}
  \partial_\theta \partial_\phi \bar H_x 
    &= \delta_x \bar G^{-1} \left( 
    (\partial_\theta \bar G) \bar G^{-1} (\partial_\phi \bar G)
    + (\partial_\phi \bar G) \bar G^{-1} (\partial_\theta \bar G)
    - (\partial_\phi \partial_\theta \bar G)
    \right) \bar G^{-1} (-\bone) \\
    &= \delta_x \bar G^{-1} \left\{
    (\partial_\theta \bar G) ( \partial_\phi \bar H ) 
    + (\partial_\phi \bar G) ( \partial_\theta \bar H )
    - (\partial_\phi \partial_\theta \bar G) \bar H
    \right\}
\end{align}

Now since
\begin{align}
  \partial_\theta (H_{x_i,A_j} - T_{ij})^2 
    &= 2 ( \partial_\theta H_{x_i,A_j} )(H_{x_i,A_j} - T_{ij})
\end{align}
and
\begin{align}
  \partial_\theta \partial_\phi (H_{x_i,A_j} - T_{ij})^2 
    &= 2 \left\{
    ( \partial_\theta \partial_\phi H_{x_i,A_j} )(H_{x_i,A_j} - T_{ij})
    + ( \partial_\theta H_{x_i,A_j} ) ( \partial_\phi H_{x_i,A_j} ) \right\} ,
\end{align}
we can use the expressions for the derivatives of $G$ above to find the derivative of $L$.
Note that if the optimum is achievable, i.e.\ $H_{x_i,A_j} = T_{ij}$ for all $i$ and $j$, 
then the second derivative is the inner product of the first derivatives of $H_{x_i,A_j}$,
and so it is the Gram matrix of these first derivatives that determine the degeneracy of $L$.

\end{document}

