\documentclass{article}
\usepackage{amsmath,amssymb}
\usepackage{fullpage}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\bone}{\mathbf{1}}

\begin{document}

Suppose that 
\begin{align}
  G = \sum_k \alpha_k A_k,
\end{align}
where $\alpha \in \R^n$
and each $A_k$ is a generator matrix,
i.e.\ $(A_k)_{xy} > 0$ for $x\neq y$ and $A_k \bone = 0$ for each $k$.

Consider a random walk driven by $G$, and let
\begin{align}
  H_{xy} = \E^x[\tau_y] = \text{ ( mean hitting time of $y$ from $x$ ) } .
\end{align}
Then  $H_{xy} \ge 0$ and
\begin{align}
  \sum_x G_{zx} H_{xy} &= -1 \quad \text{ for each } z \neq y \\
        H_{yy} = 0 .
\end{align}
Note that $\sum_x G_{yx} H_{xy} \ge 0$.

%%%%%%%%% %%%%%%%%%%
\subsection*{Gradient Ascent}

Now suppose that we are only given $T_{ij} = H_{y_i y_j}$,
and would like to find $\alpha$ to minimize the mean squared discrepancy to these.
Under a given set of $\alpha$, the vector
\begin{align}
  \hat H(\alpha)_{x y} = - \delta_x (G(\alpha)^{-y})^{-1} \bone ,
\end{align}
where $\delta_x$ is the unit vector with a $1$ in the $x$ position,
$G(\alpha) = \sum_k \alpha_k A_k$ 
and $X^{-y}$ denotes the matrix $X$ with the rows and columns corresponding to $y$ removed.
Then, we'd like to minimize
\begin{align}
  L(\alpha) = \sum_{ij} | \hat H(\alpha)_{y_i y_j} - T_{ij} |^2 .
\end{align}

Since $\partial_a X^{-1} = X^{-1} (\partial_a X )X^{-1}$,
\begin{align}
  \partial_{\alpha_k} \hat H(\alpha)_{xy} &= - \sum_z \partial_{\alpha_k} (G(\alpha)^{-y})^{-1}_{xz}  \\
  &= - \sum_z \left( (G(\alpha)^{-y})^{-1} \left( \partial_{\alpha_k} G(\alpha)^{-y} \right) (G(\alpha)^{-y})^{-1} \right)_{xz} \\
  &= - \sum_z \left( (G(\alpha)^{-y})^{-1} (A_k)^{-y} (G(\alpha)^{-y})^{-1} \right)_{xz} \\
  &= - \delta_x (G(\alpha)^{-y})^{-1} (A_k)^{-y} (G(\alpha)^{-y})^{-1} \bone \\
  &= \delta_x (G(\alpha)^{-y})^{-1} (A_k)^{-y} \hat H(\alpha)_{\cdot y},
\end{align}
where $X_{\cdot y_j}$ is the $y_j$ column of $X$,
and so the gradient of $L$ is
\begin{align}
  \partial_{\alpha_k} L(\alpha) &= \sum_{ij} 2 \left( \partial_{\alpha_k} \hat H(\alpha)_{y_i y_j} \right) \left( \hat H(\alpha)_{y_i y_j} - T_{ij} \right) \\
  &= \sum_{ij} 2 \left( \partial_{\alpha_k} \hat H(\alpha)_{y_i y_j} \right) \left( \hat H(\alpha)_{y_i y_j} - T_{ij} \right) \\
  &= -2 \sum_{ij} \left( \delta_{y_i} (G(\alpha)^{-y_j})^{-1} (A_k)^{-y_j} \hat H(\alpha)_{\cdot y_j} \right) \left( \hat H(\alpha)_{y_i y_j} - T_{ij} \right)
\end{align}
If we let
\begin{align}
  D(\alpha)_{ij} = - \left( \delta_{y_i} (G(\alpha)^{-y_j})^{-1} (A_k)^{-y_j} (G(\alpha)^{-y_j})^{-1} \bone \right)
\end{align}
and abuse notation to treat $\hat H(\alpha)_{ij}$ as an $m \times m$ matrix, then this is
\begin{align}
  \partial_{\alpha_k} L(\alpha) &= \sum_{ij} ( \hat H(\alpha)_{ij} - T_{ij} ) D(\alpha)_{ij}
\end{align}

\paragraph{Note:}
In computing the gradient, we can re-use computation of $\hat H$,
but still have to solve another linear equation involving $G(\alpha)$,
for each layer.
Also note: requires us to use the full square, symmetric matrix of $H$;
we could use a rectangular subset of it, reducing the number of inversions that need to be done.


%%%%%% %%%%%%%%
\subsection*{Given full data}

Now suppose we are given the $A_k$ and $H_{xy_i}$ for all $x$ and some set of $\{y_i\}_{i=1}^m$.
(Not what we actually have, but carry on.)
We could find $\alpha$ to minimize
\begin{align}
  F(\alpha) = \sum_{i=1}^m \sum_{z \neq y_i} \left( \sum_k \alpha_k \sum_x (A_k)_{zx} H_{xy_i} + 1 \right)^2 .
\end{align}
Differentiating this with respect to $\alpha_\ell$,
we get
\begin{align}
  \partial_{\alpha_\ell} F(\alpha) = 2 \sum_{i=1}^m \sum_{z \neq y_i} \left( \sum_x (A_\ell)_{zx} H_{xy_i} \right) \left( \sum_k \alpha_k \sum_x (A_k)_{zx} H_{xy_i} + 1 \right) .
\end{align}
For each $i$ and $\ell$ let $B^{i\ell}$ be the vector
\begin{align}
  B^{i\ell}_z = \sum_x (A_\ell)_{zx} H_{xy_i} ,
\end{align}
and let $B^{i\ell} \cdot B^{ik} = \sum_{z \neq y_i} B^{i\ell}_z B^{ik}_z$, etcetera.
The derivative above is then
\begin{align}
  \partial_{\alpha_\ell} F(\alpha) = 2 \sum_{i=1}^m \left( \sum_k \alpha_k B^{i\ell} \cdot B^{ik} + B^{i\ell} \cdot \bone \right) .
\end{align}
Setting these to zero for each $\ell$ results in the system of linear equations
\begin{align}
  Q \alpha = b
\end{align}
where
\begin{align}
    Q_{jk} &= \sum_{i=1}^m  B^{i\ell} \cdot B^{ik} \\
    b_\ell  &= - \sum_{i=1}^m  B^{i\ell } \cdot \bone .
\end{align}

For computation, we could compute the matrix
\begin{align}
  B^\ell  = A_\ell  H ,
\end{align}
indexed by $z$ and $i$,
and then set
\begin{align}
  B^\ell _{y_i i} := 0 \quad \text{for each } i ,
\end{align}
and then $C^\ell  = B^\ell  \bone$, i.e.
\begin{align}
  C^\ell _z = \sum_{i=1}^m B^\ell_{z i} .
\end{align}
Then we would have
\begin{align}
    Q_{\ell k} &= C_\ell \cdot C_k \\
    b_\ell &= C_\ell \cdot \bone' .
\end{align}


%%%%%%%% %%%%%%%%%
\subsection*{Interpolation}

Of course, we only have $T$, not $H$.
One approach would be to interpolate $T$ to get an approximation of $H$,
then proceed as above, pretending we have the correct $H$.
Then, given estimates of $\alpha$, we could possibly improve our interpolation,
and iterate.

To that end, suppose that we are given $G$ and $T$ and estimate $H$ by:
\begin{align}
  \tilde H_{x,y_i} &= \sum_{k=1}^K \lambda_{ik} v_{k}(x-y_i) \\
  &= \sum_{k=1}^K \lambda_{ik} v_{ik}(x) ,
\end{align}
where the $v_{ik}$ are given, and we want to choose $\lambda_{ik}$ to minimize
\begin{align}
  I(\lambda) &= \sum_{i=1}^m \sum_{x \neq y_i} \left( \sum_z G_{xz} \tilde H_{z y_i} + 1 \right)^2  + \gamma \sum_{ji} ( \tilde H(y_j,y_i) - T_{ji} )^2\\
  &=\sum_{i=1}^m \sum_{x \neq y_i} \left( \sum_{k=1}^K \lambda_{ik} (Gv_{ik})_{x y_i}  + 1 \right)^2  
  + \gamma \sum_{ji} ( \sum_{k=1}^K \lambda_{ik} \left(Gv_{ik})_{y_j y_i} - T_{ji} \right)^2,
\end{align}
where $\gamma$ should be strong enough that $\tilde H_{y_j,y_i}$ is close to $T_{ji}$.
Then
\begin{align}
  \partial_{\lambda_{uv}} I(\lambda)  &=
  2 \sum_{x \neq y_u} ( G v_{uv} )_{x y_u} \left( \sum_{k=1}^K \lambda_{uk} (Gv_{uk})_{x y_u}  + 1 \right)  
  + 2 \gamma \sum_{j} (G v_{uv})_{y_v y_u} \left( \sum_{k=1}^K \lambda_{uk} (Gv_{uk})_{y_j y_u} - T_{ju} \right)  \\
  &= 2 \sum_{k=1}^K \lambda_{uk} \left\{ 
  \sum_{x \neq y_u} ( G v_{uv} )_{x y_u} (Gv_{uk})_{x y_u}   
  + \gamma \sum_{j} (G v_{uv})_{y_v y_u} (Gv_{uk})_{y_j y_u}  \right\} \\
  & \qquad {}
  + 2 \sum_{x \neq y_u} ( G v_{uv} )_{x y_u} - 2 \gamma \sum_{j} (G v_{uv})_{y_v y_u} T_{ju} 
\end{align}
Note that the equations that $\lambda$ solve decouple across $u$, 
so there are $m$ systems of $K$ linear equations that need to be solved.
Also note that if we precompute the $A_\ell v_{ik}$ then we can work directly with the $\alpha$ and $\lambda$.


\paragraph{Even simpler,}
we could solve the following problem:
define $P$ to be the projection matrix so that $T = PH$.
Then for each $i$ we want to solve the following problem:
\begin{align}
    \text{minimize } & \| Q^{-y_i} z + \bone \|^2 + \gamma \| Pz - H_{\cdot y_i} \|^2 \\
    \text{subject to } &  z \ge 0 .
\end{align}

This works ridiculously well.

In some cases, so does solving:
\begin{align}
    \text{minimize } & \| Q^{-y_i} z \|^2 + \gamma \| Pz - H_{\cdot y_i} \|^2 \\
    \text{subject to } &  z \ge 0 .
\end{align}
This is because the function that is harmonic off of $\{y_j\}$
and equal to the mean hitting time of $y_i$ at those locations
is very nearly the minimal solution to this problem.
This only underestimates the truth by the mean time to first hit one of the $\{y_j\}$.

\end{document}
