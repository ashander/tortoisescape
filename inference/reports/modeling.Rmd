```{r setup, include=FALSE}
source("../resistance-fns.R")
require(parallel)
require(colorspace)
require(raster)
fig.dim <- 5
opts_chunk$set(fig.width=2*fig.dim,fig.align='center')
inflate <- function (...,fac=.2) { xr <- range(...,na.rm=TRUE); mean(xr) + (1+fac) * (xr-mean(xr)) }
```

A landscape model
=================

We model the movement on the landscape as a function of the various landscape layers.
These and related setup are stored in a JSON configuration file:

```{r layer_setup}
config <- read.json.config("report-config.json")
layer.names <- config$layer_names
for (x in config$setup_files) {
    show(load( x ) )
}
```

Everything is stored in order of the nonmissing entries of the raster layers;
let's set up a quick plotting function for these data,
and look at the layers.
Note that by default this helper function `ph()` truncates everything 20% outside of the range
of the values at the sample locations; we'll turn that off for this plot.
For future use we'll also get some other things out of here.

```{r plot_layers, fig.height=3*fig.dim}
ph <- plot.ht.fn("../../geolayers/multigrid/256x/crm_",nonmissing,homedir="../..",default.par.args=list(mar=c(3,3,3,3)+.1))
layout(matrix(1:6,nrow=3))
for (k in seq_along(layer.names)) {
    ph(layers[,k],main=layer.names[k],do.lims=FALSE)
}
layer <- with( environment(ph), layer )
sample.coords <- with(environment(ph),coordinates(tort.coords.rasterGCS))
```

The parameters determine the model.
The parameters are
```{r show_params}
params <- paramvec(config)
params
```
The first parameter, `beta`, is an overall multiplier (by `exp(beta)`).

The second set of parameters, `gamma`, determine the stationary distribution as a logistic transform
of a linear combination of the layers:
```{r show_gamma, fig.height=fig.dim}
gamma <- params[2:(1+length(layer.names))]
stationary.base <- rowSums( layers * gamma[col(layers)] )
stationary.dist <- 1 / ( 1 + exp( -stationary.base ) )
layout(t(1:2))
ph( stationary.base, main="beta, linear combination" )
ph( stationary.dist, main="stationary distribution", do.lims=FALSE )
```

The third set of parameters, `delta`,
determine relative jump rates between adjacent cells,
again by a logistic transform of a linear combination:
larger numbers mean higher jump rate.
These parameters need to be put together into the entries of the generator matrix `G`,
which then allows us to also visualize total jump rates out of each cell:
```{r show_delta, fig.height=fig.dim}
delta <- params[1+length(layer.names)+(1:length(layer.names))]
jump.base <- rowSums( layers * delta[col(layers)] )
G@x <- update.G(params)
layout(t(1:2))
ph( jump.base, main="delta, linear combination" )
ph( rowSums(G), main="total jump rate", do.lims=FALSE )
```

Now we can compute hitting times of each raster cell to a neighborhood about each sampled point.
These look, for instance, like this:
(note that labels are not, at present tortoise ID numbers)
```{r hts, fig.height=2*fig.dim, cache=TRUE}
hts <- hitting.analytic(neighborhoods,G)
plot.inds <- c(1,2,16,23,27,54,77,98,107)
layout(matrix(1:9,nrow=3))
for (k in plot.inds) {
    ph( hts[,k], main=k )
}
```


Removing a chunk from the landscape
===================================

We'd like to know how much removing a chunk of some size from the landscape would affect hitting times.
Here's how to examine that.
First, let's identify all the raster cells within 15km of a given location
```{r remove_piece, fig.height=fig.dim, fig.width=fig.dim}
footprint.center <- matrix( c( -1.8e6, -3e5 ), ncol=2 )  # in real coordinates
footprint <- get.neighborhoods( 15e3, footprint.center, nonmissing, layer )
# here is the removed bit
ph( ifelse(1:nrow(layers)%in%footprint[[1]],1,0), do.lims=FALSE, main="removed bit", legend=FALSE )
```
and see how that affects a few hitting times:
```{r remove_hts, fig.height=4/3*fig.dim}
zeroed.hts <- hitting.analytic(neighborhoods[plot.inds],G,blocked=footprint)
# and here is how it has changed a few hitting times
layout(matrix(1:6,nrow=2,byrow=TRUE))
ph( hts[,plot.inds[3]], main="before", zlim=inflate(c(hts[locs,plot.inds[3]],zeroed.hts[locs,3])) )
ph( zeroed.hts[,3], main="after", zlim=inflate(c(hts[locs,plot.inds[3]],zeroed.hts[locs,3])) )
ph( zeroed.hts[,3]-hts[,plot.inds[3]], main="diff" )
ph( hts[,plot.inds[8]], main="before", zlim=inflate(c(hts[locs,plot.inds[8]],zeroed.hts[locs,8])) )
ph( zeroed.hts[,8], main="after", zlim=inflate(c(hts[locs,plot.inds[8]],zeroed.hts[locs,8])) )
ph( zeroed.hts[,8]-hts[,plot.inds[8]], main="diff" )
```

But, we need a way of visualizing overall impact of removing a chunk.
Here are two ways to do this:
- For each point, plot the average change in mean hitting time to a bunch of other locations on the landscape.
- Draw lines between pairs of points whose hitting times increase by a certain amount.

Let's try it out the first one.
First, pick a subset of sample locations whose 15km neighborhoods are nonoverlapping,
and none of which are in the removed chunk:
```{r get_nonoverlapping, fig.width=fig.dim, fig.height=fig.dim}
ref.inds <- which.nonoverlapping(neighborhoods)
ref.inds <- ref.inds[ ! locs[ref.inds] %in% footprint[[1]] ]
ph( ifelse(1:nrow(layers)%in%unlist(neighborhoods[ref.inds]),1,0), main="reference locations" )
```
There are `r length(ref.inds)` of these.
Now compute, and plot, average change in mean hitting time:
first, absolute change; then relative change.
```{r remove_diff, fig.width=2*fig.dim, fig.height=fig.dim}
zeroed.hts <- hitting.analytic(neighborhoods[ref.inds],G,blocked=footprint)
diff.hts <- (zeroed.hts - hts[,ref.inds])
layout(t(1:2))
ph( rowMeans(diff.hts), main="mean hitting time difference" )
ph( rowMeans(diff.hts)/rowMeans(hts[,ref.inds]), main="relative hitting time difference" )
```
That looks sensible.

Now, lets put lines on top of that between pairs of the reference locations
that are in the 2.5% tails of the distribution,
colored by whether they increase or decrease.
```{r remove_tails, fig.height=fig.dim}
layout(t(1:2))
hist(diff.hts[locs[ref.inds],], main="hitting time differences")
tails <- quantile(diff.hts[locs[ref.inds],],c(0.025,0.975))
abline(v=tails,col=c("blue","red"))
lower.tails <- arrayInd( which( diff.hts[locs[ref.inds],] < tails[1] ), .dim=dim(diff.hts[locs[ref.inds],]) )
lower.lines <- LinesFromIndices( lower.tails, sample.coords, layer )
upper.tails <- arrayInd( which( diff.hts[locs[ref.inds],] > tails[2] ), .dim=dim(diff.hts[locs[ref.inds],]) )
upper.lines <- LinesFromIndices( upper.tails, sample.coords, layer )
ph( rowMeans(diff.hts), main="mean hitting time difference" )
lines( lower.lines, col='blue' )
lines( upper.lines, col='red' )
```
So, removing that chunk has mostly affected the location that's right next to the removed chunk.


Comparing the effects of several chunks
=======================================

Now, let's compare the effects of removing several chunks.
Computing hitting times above once a chunk is removed takes `r system.time( {hitting.analytic(neighborhoods[ref.inds],G,blocked=footprint) } )["elapsed"]` seconds,
so we can't do *too* many of them at once.
First, let's pick the centers of some footprints:
```{r many_footprints, fig.width=fig.dim, fig.height=fig.dim}
footprint.centers <- expand.grid( x=seq(-19e5,-17e5, length.out=5), y=seq(-2e5,-4e5,length.out=5) )
footprints <- get.neighborhoods( 15e3, footprint.centers, nonmissing, layer )
ph( ifelse(1:nrow(layers)%in%unlist(footprints),1,0), do.lims=FALSE, main="removed bits", legend=FALSE )
```
and compute hitting times
```{r many_footprints_hts, fig.width=4*fig.dim, fig.height=4*fig.dim, cache=TRUE}
do.ref.inds <- lapply( footprints, function (fp) { ref.inds[ ! locs[ref.inds] %in% fp ] } )
diff.ht.list <- lapply( seq_along(footprints), function (k) { 
        hitting.analytic(neighborhoods[do.ref.inds[[k]]],G,blocked=footprints[[k]]) - hts[,do.ref.inds[[k]]] 
    } )
mean.diffs <- sapply( diff.ht.list, rowMeans )
for (k in 1:ncol(mean.diffs)) { mean.diffs[footprints[[k]],k] <- NA }
zlims <- inflate( mean.diffs[locs[ref.inds],] )
layout( matrix(1:25,nrow=5,byrow=TRUE) )
for (k in seq_along(footprints)) {
    ph( mean.diffs[,k], par.args=list(mar=c(0,0,0,0)+.1), zlim=zlims, legend=(((k-1)%%5)==4) )
}
```

We see that reomving some chunks has a larger effect (those maps with more contrasting colors)
and others do not (those that are uniformly pink).


Identifying basins : a curiousity?
==================================

[Corollary 2.10 in Aldous & Fill](http://www.stat.berkeley.edu/~aldous/RWG/Book_Ralph/Ch2.S2.html#Ch2.ThmLemma10) implies that for any locations $i$, $j$, and $k$,
with $i \neq k$ and $j \neq k$, that the probability, beginning at $i$, that $j$ is reached before $k$,
is $(E_i T_k + E_k T_j - E_i T_j) / ( E_j T_k + E_k T_j )$,
where $E_i T_k$ is the mean hitting time of $k$ from $i$.
We cannot precisely apply this, as we compute mean hitting times to sets of states
rather than single ones,
but what we do compute should be close.

In other words, for any pair of reference locations $j$ and $k$,
we can ask which points are *closer* to $j$ than $k$ in this sense.
Let's see what that does.
```{r hitting_probs, fig.height=fig.dim, fig.width=3*fig.dim}
hit.first <- function (hts, locs, j, k) {
    # probability that j is hit before k as a function of initial location
    ( hts[,k] + hts[locs[k],j] - hts[,j] ) / ( hts[locs[j],k] + hts[locs[k],j] )
}
hit.probs <- cbind(
        hit.first( hts, locs, 27, 77 ),
        hit.first( hts, locs, 77, 98 ),
        hit.first( hts, locs, 98, 27 )
    )
layout(t(1:3))
for (k in 1:3) { ph( hit.probs[,k], do.lims=FALSE ) }
```

The resulting values range between `r min(hit.probs)` and `r max(hit.probs)`,
so are not quite probabilities,
because of using hitting times of sets rather than points,
but the discrepancy is minor.



Sensitivity of hitting times to the parameters
==============================================

Now, we only observe hitting times between the observed locations (points on the plots).
We can compute how much each parameter affects those hitting times.
First we compute the derivative of each hitting time plotted above with respect to each parameter,
which we can then visualize.
Here, rows are parameters; columns are hitting times to individuals shown in the previous plot;
white is zero, blue colors are negative, and red colors are positive.
```{r sensitivity, fig.width=4*fig.dim, fig.height=4*fig.dim, cache=TRUE}
hs <- hitting.sensitivity(params, neighborhoods[plot.inds], G, update.G, layers, transfn, valfn, ndelta, ngamma)
obs.hs <- sapply( hs, function (x) { x[locs,] } )
layout(matrix(1:(length(params)*(length(plot.inds)+1)),nrow=length(params),byrow=TRUE))
par(mar=c(0,0,0,0)+.1)
for (j in seq_along(params)) {
    plot(c(0,2), c(0,2), type='n', xaxt='n', yaxt='n')
    text(1,1,label=names(params)[j],offset=0)
    for (k in seq_along(plot.inds)) {
        ph( hs[[j]][,k], xaxt='n', yaxt='n', par.args=list(mar=c(0,0,0,0)+.1), legend=FALSE, 
            zlim=1.2*c(-1,1)*max(abs(obs.hs)), col=diverge_hcl(255) )
    }
}
```
We see the hitting times are least sensitive to the `delta` parameters,
and that hitting times to the second individual (who is isolated) are the most sensitive.



Sensitivity of parameters to the hitting times
==============================================

Suppose we know, without error,
pairwise hitting times between the reference sampling locations.
Do these uniquely determine the parameters?
Below, we vary each parameter, with the others fixed at their true values,
and plot the mean squared difference between the hitting times at the new parameter value
and at the original, "true" value.
```{r parameter_basin, cache=TRUE}
d.hts <- function (params) {
    G@x <- update.G(params)
    sum( (hitting.analytic(neighborhoods[ref.inds],G)[locs[ref.inds],] - hts[locs[ref.inds],ref.inds])^2 )
}
c.beta <- compute.plot.nearby( d.hts, params, fac=.1, do.params=1 )
plot.nearby( computed=c.beta )
c.other <- compute.plot.nearby( d.hts, params, fac=.1, do.params=seq_along(params)[-1] )
layout( matrix(1:12,ncol=3) )
plot.nearby( computed=c.other )
```

