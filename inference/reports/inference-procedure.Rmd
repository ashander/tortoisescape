```{r setup, include=FALSE}
source("../resistance-fns.R")
require(parallel)
require(colorspace)
require(raster)
fig.dim <- 5
opts_chunk$set(fig.width=2*fig.dim,fig.align='center')
inflate <- function (...,fac=.2) { xr <- range(...,na.rm=TRUE); mean(xr) + (1+fac) * (xr-mean(xr)) }
# read config
config <- read.json.config("report-config.json")
layer.names <- config$layer_names
for (x in config$setup_files) { load( x ) }
# set up 
# plotting stuff
ph <- plot.ht.fn("../../geolayers/multigrid/256x/crm_",nonmissing,homedir="../..",default.par.args=list(mar=c(3,3,3,3)+.1))
layer <- with( environment(ph), layer )
sample.coords <- with(environment(ph),coordinates(tort.coords.rasterGCS))
```

Proposed inference method
=========================

The proposed method to infer parameters, given observed hitting times $T_{x_i,j}$,
is:
0. Pick some starting parameters.
1. With these parameters, interpolate observed hitting times to the rest of the landscape.
2. Find the parameters that best fit these full hitting times.


Hitting time interpolation
==========================

First, compute the full matrix of true hitting times, which we assume are observed only on a well-spread-out subset of the space:
```{r true_hts, cache=TRUE}
ref.inds <- which.nonoverlapping(neighborhoods)
true.params <- paramvec(config)
true.G <- G
true.G@x <- update.G(true.params)
true.hts <- hitting.analytic(neighborhoods[ref.inds],true.G)
#  and subset out the observed ones:
true.obs.hts <- true.hts[locs[ref.inds],]
```

True parameters
---------------

Now, to interpolate these.
The interpolation depends on a parameter $0 \le \alpha < \infty$
that controls how much the interpolation tries to match the observed values ($\alpha \to \infty$)
or tries only to find valid hitting times ($\alpha=0$).
First, check that we get back the correct hitting times with no noise at the correct parameters and $\alpha=0$:
```{r interp_hts, fig.width=2*fig.dim, fig.height=fig.dim, cache=TRUE}
interp.hts.0 <- interp.hitting( neighborhoods[ref.inds], true.G, true.obs.hts, obs.locs=locs[ref.inds], alpha=0 )
interp.hts.1 <- interp.hitting( neighborhoods[ref.inds], true.G, true.obs.hts, obs.locs=locs[ref.inds], alpha=1 )
# looks pretty good
layout(t(1:2))
plot(1+as.vector(true.hts), 1+as.vector(interp.hts.0), log='xy', xlab="true hitting times", ylab="interpolated hitting times", main=expression(alpha==0)); abline(0,1)
plot(1+as.vector(true.hts), 1+as.vector(interp.hts.1), log='xy', xlab="true hitting times", ylab="interpolated hitting times", main=expression(alpha==1)); abline(0,1)
```
For $\alpha=0$, the relative difference is not zero, but between 
`r min((true.hts-interp.hts.0)/true.hts, na.rm=TRUE )`
and
`r max((true.hts-interp.hts.0)/true.hts, na.rm=TRUE )`,
and restricted to neighborhoods of the observed locations, is between
`r min(((true.hts-interp.hts.0)/true.hts)[ref.inds], na.rm=TRUE )`
and
`r max(((true.hts-interp.hts.0)/true.hts)[ref.inds], na.rm=TRUE )`.
For $\alpha=1$, it is between
`r min((true.hts-interp.hts.1)/true.hts, na.rm=TRUE )`
and
`r max((true.hts-interp.hts.1)/true.hts, na.rm=TRUE )`,
and restricted to neighborhoods of the observed locations, is between
`r min(((true.hts-interp.hts.1)/true.hts)[ref.inds], na.rm=TRUE )`
and
`r max(((true.hts-interp.hts.1)/true.hts)[ref.inds], na.rm=TRUE )`.
Here's what the difference in hitting times look like for two chosen individuals on the map:
```{r interp_hts_0, fig.width=2*fig.dim, fig.height=fig.dim}
check.inds <- c(9,19)
layout(t(1:2))
for (k in check.inds) { ph( interp.hts.0[,k]-true.hts[,k], zlim.fac=10, main=expression(paste("difference, ", alpha==0)) ) }
```
```{r interp_hts_1, fig.width=2*fig.dim, fig.height=fig.dim}
layout(t(1:2))
for (k in check.inds) { ph( interp.hts.0[,k]-true.hts[,k], zlim.fac=10, main=expression(paste("difference, ", alpha==0)) ) }
```

Wrong initial parameters
------------------------

Now, what if we interpolate, but using the wrong parameters?

Here's some randomly chosen parameters (rounded, and after trying a few that gave singular matrices),
and what hitting times look like for them:
```{r op_hts, fig.width=2*fig.dim, fig.height=fig.dim, cache=TRUE}
# init.params <- c( 1.9, -1.4, -1.0, 1.6, 0.4, -2.1, 2.8, -2.0, -0.2, 0.5, -0.5, -2.1, -0.9 )  # these make a singular matrix
init.params.1 <- c( 1.5, -0.3, 1.0, 0.8, -1.2, -0.4, 1.3, -2.7, 1.8, -1.1, 0.1, -1.0, 0.7 )
G@x <- update.G(init.params.1)
check.inds <- c(9,19)
wrong.hts <- hitting.analytic( neighborhoods[ref.inds[check.inds]], G )
layout(matrix(1:4,nrow=2))
ph( true.hts[,check.inds[1]], main='true hitting times' )
ph( wrong.hts[,1], main='hitting times with wrong initial params' )
ph( true.hts[,check.inds[2]], main='true hitting times' )
ph( wrong.hts[,2], main='hitting times with wrong initial params' )
```
Not totally dissimilar, but pretty different.

Now, interpolation:
```{r opt_interp_hts, fig.height=fig.dim, cache=TRUE}
op.interp.hts.0 <- interp.hitting( neighborhoods[ref.inds], G, true.obs.hts, obs.locs=locs[ref.inds], alpha=0 )
op.interp.hts.1 <- interp.hitting( neighborhoods[ref.inds], G, true.obs.hts, obs.locs=locs[ref.inds], alpha=1 )
# how do these compare to the truth?
plot.these <- sample.int(length(true.hts),1e4)
layout(t(1:2))
plot(1+true.hts[plot.these], 1+op.interp.hts.0[plot.these], log='xy', pch=20, col=adjustcolor("black",.2), xlab="true hitting times", ylab="interpolated hitting times", main=expression(alpha==0)); abline(0,1)
points( 1+as.vector(true.hts[locs[ref.inds],]), 1+as.vector(op.interp.hts.0[locs[ref.inds],]), col='red', pch=20 )
plot(1+true.hts[plot.these], 1+op.interp.hts.1[plot.these], log='xy', pch=20, col=adjustcolor("black",.2), xlab="true hitting times", ylab="interpolated hitting times", main=expression(alpha==1)); abline(0,1)
points( 1+as.vector(true.hts[locs[ref.inds],]), 1+as.vector(op.interp.hts.1[locs[ref.inds],]), col='red', pch=20 )
```
It looks like $\alpha$ is doing what it's supposed to.  
We'll see if those interpolated hitting times are good enought to get better estimates on the parameters.


Parameter inference from interpolated hitting times
===================================================

True hitting times
------------------

First, let's try inferring parameters
using the hitting times interpolated using the true parameters (basically, the truth).
Using the `trust` package to minimize the loss function:
```{r try_trust, cache=TRUE}
require(trust)
init.params <- c( 1.9, -1.4, -1.0, 1.6, 0.4, -2.1, 2.8, -2.0, -0.2, 0.5, -0.5, -2.1, -0.9 )
zeros <- unlist(neighborhoods[ref.inds]) + rep((seq_along(neighborhoods[ref.inds])-1)*nrow(G),sapply(neighborhoods[ref.inds],length))
sc.one <- 1
LddL <- logistic.trust.setup(init.params,G,update.G,interp.hts.1,zeros,sc.one,layers,transfn,valfn,ndelta,ngamma)
trust.optim <- trust( objfun=LddL, parinit=init.params, rinit=0.25, rmax=5, iterlim=100, blather=TRUE )
layout(t(1:2))
matplot( sweep(trust.optim$argpath,2,true.params,"-"), type='l', xlab="step number", ylab="parameter difference to the truth" )
plot( true.params, trust.optim$argument, xlab="true parameters", ylab="inferred" )
abline(0,1)
```

Well, gee, that works quite nicely.

Let's try it on another set of starting values.
```{r try_trust_2, cache=TRUE}
init.params.2 <- true.params + rnorm(length(true.params))
trust.optim.2 <- trust( objfun=LddL, parinit=init.params.2, rinit=0.25, rmax=5, iterlim=100, blather=TRUE )
layout(t(1:2))
matplot( sweep(trust.optim.2$argpath,2,true.params,"-"), type='l', xlab="step number", ylab="parameter difference to the truth" )
plot( true.params, trust.optim.2$argument, xlab="true parameters", ylab="inferred" )
abline(0,1)
```

Hum, yes.  I have a new favorite optimization procedure.


Wrong parameters
----------------

Now let's try this out with hitting times interpolated from the wrong parameters.
```{r wrong_trust, cache=TRUE}
init.params.1 <- c( 1.5, -0.3, 1.0, 0.8, -1.2, -0.4, 1.3, -2.7, 1.8, -1.1, 0.1, -1.0, 0.7 )  # used to get op.interp.hts.1
G@x <- update.G(init.params.1)
LddL <- logistic.trust.setup(init.params.1,G,update.G,op.interp.hts.1,zeros,sc.one,layers,transfn,valfn,ndelta,ngamma)
trust.optim <- trust( objfun=LddL, parinit=init.params.1, rinit=0.25, rmax=5, iterlim=50, blather=TRUE )
layout(t(1:2))
matplot( sweep(trust.optim$argpath,2,true.params,"-"), type='l', xlab="step number", ylab="parameter difference to the truth",
   lty=c(1,rep(2,length(layer.names)),rep(3,length(layer.names))), col=c(1,rep(2:7,2)) )
legend("topleft", lty=1:3, legend=c(expression(beta),expression(gamma),expression(delta)))
plot( true.params, trust.optim$argument, xlab="true parameters", ylab="inferred", col=c(1,rep(2:7,2))  )
points( true.params, init.params, pch=2, cex=0.5, col=c(1,rep(2:7,2)) )
legend("topleft",pch=1:2,legend=c('inferred','initial'))
abline(0,1)
```

Ok, that converged to something, hm, that seems further away from the truth, actually.
The $\gamma$ parameters have changed and the $\delta$ parameters have not.
What's the function we're trying to minimize look like?
```{r wrong_nearby, cache=TRUE, fig.height=3*fig.dim}
layout(matrix(1:6,ncol=2))
plot.nearby( f=function(x){LddL(x)$value}, params=trust.optim$argument, fac=0.5, npoints=11, do.params=c(1,2,3,8,9,10) )
```
Huh, like we'd expect.

How did hitting times change across that optimization?
```{r wrong_ht_maps, cache=TRUE, fig.height=2*fig.dim, fig.width=3*fig.dim}
check.inds <- c(9,19)
G@x <- update.G(init.params.1)
wrong.hts <- hitting.analytic( neighborhoods[ref.inds[check.inds]], G )
G@x <- update.G(trust.optim$argument)
new.wrong.hts <- hitting.analytic( neighborhoods[ref.inds[check.inds]], G )
layout(matrix(1:6,nrow=2,byrow=TRUE))
for (k in seq_along(check.inds)) {
    ph( wrong.hts[,k], main="starting" )
    ph( new.wrong.hts[,k], main="ending" )
    ph( new.wrong.hts[,k]-wrong.hts[,k], main="diff")
}
```
OK, it's mostly making the hitting times **much** bigger.

Why is this decreasing the objective function?
The objective function is a *weighted* sum of differences; here are the weights,
and the new, interpolated hitting times multiplied by these weights:
```{r optim_weightings, fig.height=fig.dim, cache=TRUE}
new.wrong.hts <- hitting.analytic( neighborhoods[ref.inds], G )
layout(t(1:2))
ph( with(environment(LddL),weightings), main='weightings', do.lims=FALSE )
ph( with(environment(LddL),weightings)*rowMeans(new.wrong.hts), main='weighted true mean hitting times', do.lims=FALSE )
```
Doesn't look too bad.
Here's the weighted mean deviations from hitting-time-ness that's being minimized:
```{r optimizer, cache=TRUE}
fit.vector <- function (new.params) {
    G@x <- update.G(new.params)
    dG <- rowSums(G)
    GH <- G %*% with(environment(LddL),hts) - dG*with(environment(LddL),hts)
    GH[with(environment(LddL),zeros)] <- 0
    with(environment(LddL),weightings)*((GH+with(environment(LddL),sc.one))^2)
}
layout(t(1:2))
ph( rowSums(fit.vector(init.params.1)), do.lims=FALSE, main="initial fit" )
ph( rowSums(fit.vector(trust.optim$argument)), do.lims=FALSE, main="final fit" )
```
Well, that's doing the right thing.

```{r}
G@x <- update.G(trust.optim$argument)
dG <- rowSums(G)
GH <- G %*% with(environment(LddL),hts) - dG*with(environment(LddL),hts)
GH[with(environment(LddL),zeros)] <- 0
```
